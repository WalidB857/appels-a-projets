{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "# Enrichissement Hybride LLM - Seine-Saint-Denis Appels √† Projets\n",
    "\n",
    "Ce notebook scrape ET enrichit les donn√©es avec Claude Sonnet 4.5 pour extraire:\n",
    "- **Scraping complet** du site seine-saint-denis.gouv.fr (avec pagination)\n",
    "- **Filtrage** sur ann√©e N-1 (2025) et ann√©e N (2026)\n",
    "- **R√©sum√©s structur√©s** via LLM\n",
    "- **Montants (min/max)** extraits automatiquement\n",
    "- **Cat√©gories et tags** intelligents\n",
    "- **Public cible** identifi√©\n",
    "- **Modalit√©s et d√©marches** structur√©es\n",
    "- **Extraction PDF** des r√®glements\n",
    "\n",
    "**Approche:** Workflow complet et ind√©pendant = Scraping + LLM Claude pour enrichissement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 1. Imports et configuration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import tempfile\n",
    "import hashlib\n",
    "import itertools\n",
    "\n",
    "# Imports LLM\n",
    "from anthropic import Anthropic\n",
    "import pypdf"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "# Charger les variables d'environnement\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# V√©rifier Claude API key\n",
    "claude_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "if claude_api_key:\n",
    "    print(f\"‚úÖ ANTHROPIC_API_KEY trouv√©e: {claude_api_key[:10]}...\")\n",
    "else:\n",
    "    print(f\"‚ùå ANTHROPIC_API_KEY non trouv√©e dans .env\")\n",
    "    print(f\"   ‚ö†Ô∏è Vous devez ajouter: ANTHROPIC_API_KEY=sk-ant-xxxxxx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 2. Configuration scraper"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "# Configuration du scraper\n",
    "BASE_URL = \"https://www.seine-saint-denis.gouv.fr/Actualites/Appels-a-projets\"\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "}\n",
    "\n",
    "# Filtrage sur ann√©e N-1 et ann√©e N\n",
    "CURRENT_YEAR = datetime.now().year\n",
    "YEARS_TO_KEEP = [CURRENT_YEAR - 1, CURRENT_YEAR]\n",
    "\n",
    "print(f\"‚úÖ Configuration pr√™te\")\n",
    "print(f\"   Base URL: {BASE_URL}\")\n",
    "print(f\"   Ann√©es filtr√©es: {YEARS_TO_KEEP}\")\n",
    "print(f\"   D√©pendances: requests, BeautifulSoup4, pandas, anthropic, pypdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 3. Scraper les appels √† projets de Seine-Saint-Denis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "def fetch_all_pages(base_url, max_pages=5):\n",
    "    \"\"\"R√©cup√©rer toutes les pages avec pagination (offset)\"\"\"\n",
    "    all_html_pages = []\n",
    "    \n",
    "    for page_num in range(max_pages):\n",
    "        offset = page_num * 10\n",
    "        if offset == 0:\n",
    "            url = base_url\n",
    "        else:\n",
    "            url = f\"{base_url}/(offset)/{offset}\"\n",
    "        \n",
    "        print(f\"üîÑ Fetching page {page_num + 1} (offset={offset})...\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # V√©rifier s'il y a du contenu (articles ou liens)\n",
    "            articles = soup.find_all(['div', 'li', 'article'], class_=re.compile(r'(item|article|news|appel)', re.I))\n",
    "            links = soup.find_all('a', href=re.compile(r'Appels-a-projets/.+', re.I))\n",
    "            \n",
    "            if not articles and not links:\n",
    "                print(f\"   ‚ö†Ô∏è Pas de contenu pertinent trouv√©, arr√™t pagination\")\n",
    "                break\n",
    "            \n",
    "            all_html_pages.append(response.text)\n",
    "            print(f\"   ‚úÖ Page r√©cup√©r√©e ({len(response.text)} chars)\")\n",
    "            \n",
    "            time.sleep(1)  # Pause pour √™tre poli\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Erreur: {str(e)[:50]}\")\n",
    "            break\n",
    "    \n",
    "    return all_html_pages\n",
    "\n",
    "# R√©cup√©rer les pages (on limite √† 5 pages pour l'exemple, augmenter si n√©cessaire)\n",
    "html_pages = fetch_all_pages(BASE_URL, max_pages=5)\n",
    "print(f\"\\n‚úÖ {len(html_pages)} pages r√©cup√©r√©es\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "def extract_years_from_text(text):\n",
    "    \"\"\"Extraire les ann√©es mentionn√©es dans un texte\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    # Chercher 2024, 2025, 2026 etc.\n",
    "    return [int(y) for y in re.findall(r'\\b(202[0-9])\\b', text)]\n",
    "\n",
    "def scrape_ssd_aap(html_pages, years_filter=None):\n",
    "    \"\"\"Scrape les AAP du site Seine-Saint-Denis avec filtrage par ann√©e\"\"\"\n",
    "    aap_list = []\n",
    "    seen_urls = set()\n",
    "    \n",
    "    if not html_pages:\n",
    "        return aap_list\n",
    "    \n",
    "    for page_html in html_pages:\n",
    "        soup = BeautifulSoup(page_html, 'html.parser')\n",
    "        \n",
    "        # Strat√©gie 1: Chercher les blocs d'articles\n",
    "        items = soup.find_all(['div', 'article'], class_=re.compile(r'(item|article|news-item)', re.I))\n",
    "        \n",
    "        # Strat√©gie 2: Si pas d'items clairs, chercher tous les liens dans la zone de contenu principal\n",
    "        if not items:\n",
    "            main_content = soup.find('div', id=re.compile(r'(main|content|centre)', re.I))\n",
    "            if main_content:\n",
    "                items = main_content.find_all('a', href=re.compile(r'Appels-a-projets/', re.I))\n",
    "        \n",
    "        for item in items:\n",
    "            try:\n",
    "                aap = {}\n",
    "                \n",
    "                # Extraction Titre et URL\n",
    "                if item.name == 'a':\n",
    "                    title_elem = item\n",
    "                    link_elem = item\n",
    "                    container = item.parent\n",
    "                else:\n",
    "                    title_elem = item.find(['h2', 'h3', 'h4', 'a'])\n",
    "                    link_elem = item.find('a', href=True)\n",
    "                    container = item\n",
    "                \n",
    "                if not title_elem or not link_elem:\n",
    "                    continue\n",
    "                    \n",
    "                aap['titre'] = title_elem.get_text(strip=True)\n",
    "                aap['url_source'] = urljoin(BASE_URL, link_elem.get('href', ''))\n",
    "                \n",
    "                # Ignorer si ce n'est pas un lien vers un AAP (ex: lien de pagination)\n",
    "                if 'offset' in aap['url_source'] or aap['url_source'] == BASE_URL:\n",
    "                    continue\n",
    "                \n",
    "                # √âviter les doublons\n",
    "                if aap['url_source'] in seen_urls:\n",
    "                    continue\n",
    "                seen_urls.add(aap['url_source'])\n",
    "                \n",
    "                # Extraction du texte pour filtrage et r√©sum√©\n",
    "                text_content = container.get_text(' ')\n",
    "                \n",
    "                # Filtrage par ann√©e\n",
    "                if years_filter:\n",
    "                    years_found = extract_years_from_text(text_content) + extract_years_from_text(aap['titre'])\n",
    "                    # Si on trouve des ann√©es, on v√©rifie si l'une d'elles est dans notre filtre\n",
    "                    # Si aucune ann√©e trouv√©e, on garde par d√©faut (le LLM v√©rifiera)\n",
    "                    if years_found:\n",
    "                        if not any(y in years_filter for y in years_found):\n",
    "                            continue\n",
    "                \n",
    "                # R√©sum√© pr√©liminaire\n",
    "                desc_elem = container.find(['p', 'div'], class_=re.compile(r'(desc|intro|chapo)', re.I))\n",
    "                aap['resume'] = desc_elem.get_text(strip=True) if desc_elem else None\n",
    "                \n",
    "                # Dates (tentative d'extraction regex)\n",
    "                dates = re.findall(r'\\d{1,2}[/\\-]\\d{1,2}[/\\-]\\d{4}', text_content)\n",
    "                if dates:\n",
    "                    try:\n",
    "                        # On prend la derni√®re date comme date limite potentielle\n",
    "                        aap['date_limite'] = pd.to_datetime(dates[-1].replace('-', '/'), dayfirst=True).date()\n",
    "                    except:\n",
    "                        aap['date_limite'] = None\n",
    "                else:\n",
    "                    aap['date_limite'] = None\n",
    "                \n",
    "                # Champs par d√©faut\n",
    "                aap['organisme'] = 'Pr√©fecture de Seine-Saint-Denis'\n",
    "                aap['perimetre_geo'] = 'Seine-Saint-Denis (93)'\n",
    "                aap['id_record'] = f\"ssd_{datetime.now().strftime('%Y%m%d')}_{hash(aap['url_source']) % 100000}\"\n",
    "                \n",
    "                aap_list.append(aap)\n",
    "                print(f\"   ‚úÖ Trouv√©: {aap['titre'][:60]}...\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Erreur parsing item: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    return aap_list\n",
    "\n",
    "# Ex√©cuter le scraping\n",
    "aap_data = scrape_ssd_aap(html_pages, years_filter=YEARS_TO_KEEP)\n",
    "print(f\"\\n‚úÖ {len(aap_data)} appels √† projets retenus (Filtre ann√©es: {YEARS_TO_KEEP})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 4. Cr√©er et nettoyer le DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "# Cr√©er DataFrame\n",
    "if aap_data:\n",
    "    mapped_df_ssd = pd.DataFrame(aap_data)\n",
    "    print(f\"üìä DataFrame cr√©√©: {mapped_df_ssd.shape}\")\n",
    "else:\n",
    "    mapped_df_ssd = pd.DataFrame(columns=['titre', 'url_source', 'resume', 'date_limite', 'organisme'])\n",
    "    print(\"‚ö†Ô∏è Aucune donn√©e trouv√©e\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "# Fonction de nettoyage du texte\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Appliquer le nettoyage\n",
    "if not mapped_df_ssd.empty:\n",
    "    for col in mapped_df_ssd.select_dtypes(include=['object']).columns:\n",
    "        mapped_df_ssd[col] = mapped_df_ssd[col].apply(clean_text)\n",
    "\n",
    "    # Ajouter colonnes manquantes pour l'enrichissement\n",
    "    cols_to_add = ['montant_max', 'montant_min', 'public_cible', 'categories', 'mots_cles', 'objectif', 'modalite', 'demarches', 'contact', 'taux_financement']\n",
    "    for col in cols_to_add:\n",
    "        if col not in mapped_df_ssd.columns:\n",
    "            mapped_df_ssd[col] = None\n",
    "            \n",
    "    # Fingerprint unique\n",
    "    if 'fingerprint' not in mapped_df_ssd.columns:\n",
    "        mapped_df_ssd['fingerprint'] = mapped_df_ssd.apply(\n",
    "            lambda row: hashlib.md5(f\"{row.get('titre')}|{row.get('url_source')}\".encode()).hexdigest()[:12], \n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    print(\"‚úÖ DataFrame pr√©par√© pour l'enrichissement\")\n",
    "    print(f\"   Colonnes: {list(mapped_df_ssd.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 5. Fonctions pour extraction PDF"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "def extract_pdf_text(pdf_url, max_pages=3):\n",
    "    \"\"\"Extraire le texte d'un PDF depuis une URL\"\"\"\n",
    "    try:\n",
    "        response = requests.get(pdf_url, headers=HEADERS, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as tmp:\n",
    "            tmp.write(response.content)\n",
    "            tmp_path = tmp.name\n",
    "        \n",
    "        reader = pypdf.PdfReader(tmp_path)\n",
    "        text = ''\n",
    "        for page_num, page in enumerate(reader.pages[:max_pages]):\n",
    "            text += page.extract_text() + '\\n'\n",
    "        \n",
    "        os.remove(tmp_path)\n",
    "        return text if text.strip() else None\n",
    "    except Exception as e:\n",
    "        # print(f\"  ‚ö†Ô∏è Erreur PDF {pdf_url.split('/')[-1]}: {str(e)[:30]}\")\n",
    "        return None\n",
    "\n",
    "def find_pdf_links(soup, base_url):\n",
    "    \"\"\"Trouver les liens PDF dans une page\"\"\"\n",
    "    pdf_links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link.get('href', '')\n",
    "        text = link.get_text().lower()\n",
    "        \n",
    "        if ('pdf' in href.lower() or \n",
    "            any(keyword in text for keyword in ['reglement', 'document', 'cahier', 'guide', 't√©l√©charger'])):\n",
    "            full_url = urljoin(base_url, href)\n",
    "            if full_url not in pdf_links and full_url.lower().endswith('.pdf'):\n",
    "                pdf_links.append(full_url)\n",
    "    \n",
    "    return pdf_links[:2]  # Limiter √† 2 PDFs max pour ne pas surcharger\n",
    "\n",
    "print(\"‚úÖ Fonctions PDF cr√©√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 6. Classe LLMEnricher"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "class LLMEnricher:\n",
    "    \"\"\"Enrichir les donn√©es AAP avec Claude Sonnet 4.5\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key=None, model='claude-sonnet-4-5'):\n",
    "        self.api_key = api_key or os.getenv('ANTHROPIC_API_KEY')\n",
    "        self.model = model\n",
    "        self.client = Anthropic(api_key=self.api_key) if self.api_key else None\n",
    "        self.max_retries = 3\n",
    "        \n",
    "        if not self.client:\n",
    "            raise ValueError('‚ùå ANTHROPIC_API_KEY non trouv√©e')\n",
    "    \n",
    "    def extract_full_page(self, url, html_content, pdf_texts=None):\n",
    "        \"\"\"Extraire toutes les donn√©es manquantes d'une page\"\"\"\n",
    "        \n",
    "        if not self.client:\n",
    "            return None\n",
    "        \n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Nettoyer le HTML pour r√©duire la taille\n",
    "        for script in soup([\"script\", \"style\", \"nav\", \"footer\"]):\n",
    "            script.decompose()\n",
    "            \n",
    "        text_content = soup.get_text('\\n')\n",
    "        text_content = re.sub(r'\\n+', '\\n', text_content).strip()\n",
    "        \n",
    "        if pdf_texts:\n",
    "            text_content += '\\n\\n--- CONTENU DES DOCUMENTS PDF JOINTS ---\\n'\n",
    "            text_content += '\\n\\n'.join(pdf_texts)\n",
    "        \n",
    "        # Tronquer si trop long (Claude a une grande fen√™tre mais restons raisonnables)\n",
    "        text_content = text_content[:25000]\n",
    "        \n",
    "        prompt = f\"\"\"Tu es un expert en analyse d'appels √† projets publics pour la Seine-Saint-Denis.\n",
    "        \n",
    "Analyse le texte ci-dessous (page web + potentiels PDFs) et extrais les informations structur√©es en JSON.\n",
    "\n",
    "FORMAT JSON ATTENDU:\n",
    "{{\n",
    "  \"resume\": \"R√©sum√© synth√©tique en 2-3 phrases (max 400 caract√®res)\",\n",
    "  \"montant_max\": montant maximum en euros (nombre ou null), \n",
    "  \"montant_min\": montant minimum en euros (nombre ou null),\n",
    "  \"taux_financement\": \"pourcentage ou description courte (null si non trouv√©)\",\n",
    "  \"categories\": [\"liste\", \"de\", \"cat√©gories\", \"pertinentes\"],\n",
    "  \"public_cible\": [\"associations\", \"collectivit√©s\", \"entreprises\", \"particuliers\"],\n",
    "  \"mots_cles\": [\"3-5\", \"mots-cl√©s\", \"importants\"],\n",
    "  \"objectif\": \"Objectif principal de l'appel √† projet\",\n",
    "  \"modalite\": \"Conditions principales d'√©ligibilit√©\",\n",
    "  \"demarches\": \"Comment candidater (plateforme, email, dossier)\",\n",
    "  \"contact\": \"Email ou t√©l√©phone de contact (ou null)\",\n",
    "  \"date_limite\": \"YYYY-MM-DD\" (si trouv√©e dans le texte, sinon null)\n",
    "}}\n",
    "\n",
    "R√àGLES:\n",
    "- Retourne UNIQUEMENT du JSON valide.\n",
    "- Si une info n'existe pas, mets null.\n",
    "- Sois pr√©cis sur les montants.\n",
    "- Pour la date limite, essaie de trouver la date exacte de cl√¥ture.\n",
    "\n",
    "TEXTE √Ä ANALYSER:\n",
    "{text_content}\"\"\"\n",
    "        \n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                message = self.client.messages.create(\n",
    "                    model=self.model,\n",
    "                    max_tokens=1500,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )\n",
    "                \n",
    "                response_text = message.content[0].text\n",
    "                # Nettoyage du markdown json si pr√©sent\n",
    "                response_text = response_text.replace('```json', '').replace('```', '').strip()\n",
    "                \n",
    "                return json.loads(response_text)\n",
    "            except Exception as e:\n",
    "                if attempt == self.max_retries - 1:\n",
    "                    print(f\"    ‚ùå Erreur LLM apr√®s {self.max_retries} essais: {str(e)[:50]}\")\n",
    "                    return None\n",
    "                time.sleep(1)\n",
    "\n",
    "print(\"‚úÖ Classe LLMEnricher cr√©√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 7. Aper√ßu des PDFs extraits (Test)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "if not mapped_df_ssd.empty:\n",
    "    print(\"üìÑ Test extraction PDF sur le premier √©l√©ment:\")\n",
    "    row = mapped_df_ssd.iloc[0]\n",
    "    url = row['url_source']\n",
    "    print(f\"   URL: {url}\")\n",
    "    \n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        pdfs = find_pdf_links(soup, url)\n",
    "        print(f\"   PDFs trouv√©s: {len(pdfs)}\")\n",
    "        for pdf in pdfs:\n",
    "            print(f\"   - {pdf.split('/')[-1]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Erreur: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 8. Initialiser l'enrichisseur LLM"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "try:\n",
    "    enricher = LLMEnricher()\n",
    "    print(\"‚úÖ LLMEnricher initialis√© avec succ√®s\")\n",
    "except ValueError as e:\n",
    "    print(f\"‚ùå {str(e)}\")\n",
    "    enricher = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 9. Enrichir avec LLM"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "if enricher and not mapped_df_ssd.empty:\n",
    "    print(f\"üîÑ Enrichissement de {len(mapped_df_ssd)} appels √† projets...\\n\")\n",
    "    \n",
    "    for idx, row in mapped_df_ssd.iterrows():\n",
    "        url = row.get('url_source')\n",
    "        titre = str(row.get('titre', 'N/A'))[:50]\n",
    "        \n",
    "        print(f\"üîÑ [{idx+1}/{len(mapped_df_ssd)}] {titre}...\", end=' ')\n",
    "        \n",
    "        try:\n",
    "            # 1. R√©cup√©rer le contenu de la page\n",
    "            response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # 2. Chercher et extraire les PDFs\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            pdf_links = find_pdf_links(soup, url)\n",
    "            \n",
    "            pdf_texts = []\n",
    "            if pdf_links:\n",
    "                print(f\"(+{len(pdf_links)} PDFs)\", end=' ')\n",
    "                for pdf_url in pdf_links:\n",
    "                    pdf_text = extract_pdf_text(pdf_url)\n",
    "                    if pdf_text:\n",
    "                        pdf_texts.append(pdf_text[:4000]) # Limite par PDF\n",
    "            \n",
    "            # 3. Appel LLM\n",
    "            extracted = enricher.extract_full_page(url, response.text, pdf_texts)\n",
    "            \n",
    "            # 4. Mise √† jour du DataFrame\n",
    "            if extracted:\n",
    "                for key, value in extracted.items():\n",
    "                    if key in mapped_df_ssd.columns:\n",
    "                        # Si date_limite trouv√©e par LLM et vide dans le DF, on met √† jour\n",
    "                        if key == 'date_limite' and value:\n",
    "                            mapped_df_ssd.at[idx, key] = value\n",
    "                        # Pour les autres champs, on √©crase ou remplit\n",
    "                        elif value is not None:\n",
    "                            mapped_df_ssd.at[idx, key] = value\n",
    "                print(\"‚úÖ\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  (Pas de r√©ponse LLM)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {str(e)[:30]}\")\n",
    "        \n",
    "        time.sleep(1) # Rate limiting\n",
    "    \n",
    "    print(f\"\\n‚úÖ Enrichissement termin√©!\")\n",
    "else:\n",
    "    print(\"‚ùå Pas d'enrichissement (soit pas de donn√©es, soit pas de cl√© API)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 10. Statistiques et aper√ßu"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "if not mapped_df_ssd.empty:\n",
    "    print(\"üìä Statistiques de remplissage:\")\n",
    "    for col in ['montant_max', 'date_limite', 'public_cible', 'categories']:\n",
    "        filled = mapped_df_ssd[col].notna().sum()\n",
    "        print(f\"   - {col}: {filled}/{len(mapped_df_ssd)} ({filled/len(mapped_df_ssd)*100:.1f}%)\")\n",
    "        \n",
    "    print(\"\\nüìã Aper√ßu des donn√©es enrichies:\")\n",
    "    from IPython.display import display, HTML\n",
    "    display(mapped_df_ssd.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 11. Exporter les donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "if not mapped_df_ssd.empty:\n",
    "    # Export CSV\n",
    "    csv_path = '../data/ssd_aap_enriched.csv'\n",
    "    os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "    mapped_df_ssd.to_csv(csv_path, index=False)\n",
    "    print(f\"‚úÖ Donn√©es export√©es vers {csv_path}\")\n",
    "    \n",
    "    # Export JSON\n",
    "    json_path = '../data/ssd_aap_enriched.json'\n",
    "    mapped_df_ssd.to_json(json_path, orient='records', force_ascii=False, indent=2)\n",
    "    print(f\"‚úÖ Donn√©es export√©es vers {json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## 12. Upload Airtable (Optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "# if not mapped_df_ssd.empty:\n",
    "#     from appels_a_projets.connectors.airtable_connector import AirtableConnector\n",
    "#     connector = AirtableConnector()\n",
    "#     connector.upload_dataframe(mapped_df_ssd)\n",
    "#     print(\"‚úÖ Upload vers Airtable termin√©\")"
   ]
  }
 ]
}