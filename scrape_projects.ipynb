{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5a77bb9",
   "metadata": {},
   "source": [
    "# Enrichissement Hybride LLM - Seine-Saint-Denis Appels √† Projets\n",
    "\n",
    "Ce notebook scrape ET enrichit les donn√©es du site **seine-saint-denis.gouv.fr** avec Claude Sonnet 4.5.\n",
    "\n",
    "**Filtrage:** Projets publi√©s en 2025, 2026 et ult√©rieurs (>= 01/01/2025)\n",
    "\n",
    "**Approche:** Scraping complet + LLM Claude pour enrichissement structur√©"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a1fe2d",
   "metadata": {},
   "source": [
    "## 1. Imports et configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fca9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import json\n",
    "from urllib.parse import urljoin, parse_qs, urlparse\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import tempfile\n",
    "import hashlib\n",
    "import itertools\n",
    "\n",
    "# Imports LLM\n",
    "from anthropic import Anthropic\n",
    "import pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b538675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les variables d'environnement\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# V√©rifier Claude API key\n",
    "claude_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "if claude_api_key:\n",
    "    print(f\"‚úÖ ANTHROPIC_API_KEY trouv√©e: {claude_api_key[:10]}...\")\n",
    "else:\n",
    "    print(f\"‚ùå ANTHROPIC_API_KEY non trouv√©e dans .env\")\n",
    "    print(f\"   ‚ö†Ô∏è Vous devez ajouter: ANTHROPIC_API_KEY=sk-ant-xxxxxx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8816f258",
   "metadata": {},
   "source": [
    "## 2. Configuration scraper Seine-Saint-Denis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab979610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du scraper\n",
    "BASE_URL = \"https://www.seine-saint-denis.gouv.fr/Actualites/Appels-a-projets\"\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "}\n",
    "\n",
    "# Filtre de date: >= 01/01/2025\n",
    "DATE_FILTER_START = datetime(2025, 1, 1)\n",
    "CURRENT_YEAR = datetime.now().year\n",
    "\n",
    "print(f\"‚úÖ Configuration pr√™te\")\n",
    "print(f\"   Base URL: {BASE_URL}\")\n",
    "print(f\"   Filtre date: >= {DATE_FILTER_START.strftime('%d/%m/%Y')}\")\n",
    "print(f\"   Ann√©es accept√©es: 2025, 2026+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c9f8ae",
   "metadata": {},
   "source": [
    "## 3. Scraper les appels √† projets de Seine-Saint-Denis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e25da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_seine_saint_denis_aap(base_url, max_pages=5):\n",
    "    \"\"\"Scraper tous les AAP de Seine-Saint-Denis avec pagination\"\"\"\n",
    "    aap_list = []\n",
    "    page_offset = 0\n",
    "    pages_scraped = 0\n",
    "    \n",
    "    while pages_scraped < max_pages:\n",
    "        url = f\"{base_url}/(offset)/{page_offset}\"\n",
    "        print(f\"üîÑ Scraping page {pages_scraped + 1}: {url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            html_content = response.text\n",
    "            \n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            \n",
    "            # Chercher les articles/conteneurs AAP\n",
    "            article_containers = soup.find_all(['article', 'div'], class_=re.compile(r'(article|appel|item|news|post)', re.I))\n",
    "            \n",
    "            if not article_containers:\n",
    "                article_containers = soup.find_all('a', href=re.compile(r'appel|projet|actualite', re.I))\n",
    "            \n",
    "            if not article_containers:\n",
    "                print(f\"   ‚ö†Ô∏è Aucun conteneur trouv√©, fin du scraping\")\n",
    "                break\n",
    "            \n",
    "            page_items = 0\n",
    "            for container in article_containers:\n",
    "                aap = extract_aap_item(container, base_url)\n",
    "                \n",
    "                if aap and aap.get('date_publication'):\n",
    "                    try:\n",
    "                        item_date = pd.to_datetime(aap['date_publication'])\n",
    "                        if item_date >= DATE_FILTER_START:\n",
    "                            aap_list.append(aap)\n",
    "                            page_items += 1\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            print(f\"   ‚úÖ {page_items} AAP valides trouv√©s\")\n",
    "            \n",
    "            if page_items == 0 and pages_scraped > 0:\n",
    "                print(f\"   ‚ÑπÔ∏è Aucun nouvel AAP, fin du scraping\")\n",
    "                break\n",
    "            \n",
    "            page_offset += 10\n",
    "            pages_scraped += 1\n",
    "            time.sleep(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Erreur: {str(e)[:80]}\")\n",
    "            break\n",
    "    \n",
    "    return aap_list\n",
    "\n",
    "\n",
    "def extract_aap_item(container, base_url):\n",
    "    \"\"\"Extraire les donn√©es d'un seul AAP\"\"\"\n",
    "    aap = {}\n",
    "    \n",
    "    try:\n",
    "        # Titre\n",
    "        title_elem = container.find(['h2', 'h3', 'h4', 'a', 'span'])\n",
    "        if title_elem:\n",
    "            aap['titre'] = title_elem.get_text(strip=True)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        # URL\n",
    "        link = container.find('a', href=True) if container.name != 'a' else container\n",
    "        if link:\n",
    "            href = link.get('href', '')\n",
    "            aap['url_source'] = urljoin(base_url, href)\n",
    "        else:\n",
    "            aap['url_source'] = None\n",
    "        \n",
    "        # Description/r√©sum√©\n",
    "        desc = container.find(['p', 'span'], class_=re.compile(r'(desc|summary|excerpt|chapeau)', re.I))\n",
    "        if desc:\n",
    "            aap['resume'] = desc.get_text(strip=True)\n",
    "        \n",
    "        # Date de publication\n",
    "        text_content = container.get_text(' ')\n",
    "        dates = re.findall(r'\\d{1,2}[/-]\\d{1,2}[/-]\\d{4}', text_content)\n",
    "        \n",
    "        if dates:\n",
    "            try:\n",
    "                # Essayer format JJ/MM/AAAA\n",
    "                aap['date_publication'] = pd.to_datetime(dates[0], format='%d/%m/%Y').date()\n",
    "            except:\n",
    "                try:\n",
    "                    # Essayer format JJ-MM-AAAA\n",
    "                    aap['date_publication'] = pd.to_datetime(dates[0], format='%d-%m-%Y').date()\n",
    "                except:\n",
    "                    aap['date_publication'] = None\n",
    "        else:\n",
    "            aap['date_publication'] = None\n",
    "        \n",
    "        # Date limite de candidature (si trouv√©e)\n",
    "        if len(dates) >= 2:\n",
    "            try:\n",
    "                aap['date_limite'] = pd.to_datetime(dates[1], format='%d/%m/%Y').date()\n",
    "            except:\n",
    "                aap['date_limite'] = None\n",
    "        else:\n",
    "            aap['date_limite'] = None\n",
    "        \n",
    "        # Montant\n",
    "        amounts = re.findall(r'(\\d+[\\s.,]*\\d*)\\s*(?:‚Ç¨|euros?|EUROS?)', text_content, re.I)\n",
    "        if amounts:\n",
    "            try:\n",
    "                cleaned = amounts[-1].replace(' ', '').replace('.', '').replace(',', '.')\n",
    "                aap['montant_max'] = float(cleaned)\n",
    "            except:\n",
    "                aap['montant_max'] = None\n",
    "        else:\n",
    "            aap['montant_max'] = None\n",
    "        \n",
    "        # Organisme\n",
    "        aap['organisme'] = 'Seine-Saint-Denis (93)'\n",
    "        \n",
    "        # ID unique\n",
    "        aap['id_record'] = f\"ssd_{datetime.now().strftime('%Y%m%d%H%M%S')}_{hash(aap['titre']) % 10000}\"\n",
    "        \n",
    "        return aap\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Erreur extraction: {str(e)[:50]}\")\n",
    "        return None\n",
    "\n",
    "# Scraper\n",
    "print(f\"\\nüîÑ Scraping Seine-Saint-Denis...\\n\")\n",
    "aap_data = scrape_seine_saint_denis_aap(BASE_URL, max_pages=5)\n",
    "print(f\"\\n‚úÖ {len(aap_data)} appels √† projets extraits (filtr√©s 2025+)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f814c1f",
   "metadata": {},
   "source": [
    "## 4. Cr√©er et nettoyer le DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71402c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er DataFrame\n",
    "if aap_data:\n",
    "    mapped_df_ssd = pd.DataFrame(aap_data)\n",
    "    print(f\"üìä DataFrame cr√©√©: {mapped_df_ssd.shape}\")\n",
    "    print(f\"   Colonnes: {list(mapped_df_ssd.columns)}\")\n",
    "else:\n",
    "    mapped_df_ssd = pd.DataFrame()\n",
    "    print(f\"‚ö†Ô∏è Aucune donn√©e √† traiter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589ff140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de nettoyage du texte\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    html_entities = {\n",
    "        '&eacute;': '√©', '&icirc;': '√Æ', '&ag√†;': '√†', '&ocirc;': '√¥',\n",
    "        '&nbsp;': ' ', '&quot;': '\\\"', '&amp;': '&',\n",
    "        '&rsquo;': \"'\", '&ldquo;': '\\u201c', '&rdquo;': '\\u201d'\n",
    "    }\n",
    "    for entity, char in html_entities.items():\n",
    "        text = text.replace(entity, char)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Appliquer le nettoyage\n",
    "if not mapped_df_ssd.empty:\n",
    "    for col in mapped_df_ssd.select_dtypes(include=['object']).columns:\n",
    "        if col not in ['categories']:\n",
    "            mapped_df_ssd[col] = mapped_df_ssd[col].apply(clean_text)\n",
    "    print(\"‚úÖ Texte nettoy√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467a1b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter colonnes manquantes\n",
    "if not mapped_df_ssd.empty:\n",
    "    colonnes_requises = ['public_cible', 'taux_financement', 'contact', 'modalite', 'demarches', 'mots_cles', 'objectif', 'montant_min', 'note', 'tags', 'categories']\n",
    "    \n",
    "    for col in colonnes_requises:\n",
    "        if col not in mapped_df_ssd.columns:\n",
    "            mapped_df_ssd[col] = None\n",
    "    \n",
    "    if 'perimetre_geo' not in mapped_df_ssd.columns:\n",
    "        mapped_df_ssd['perimetre_geo'] = 'Seine-Saint-Denis'\n",
    "    \n",
    "    if 'fingerprint' not in mapped_df_ssd.columns:\n",
    "        def create_fingerprint(row):\n",
    "            titre = str(row['titre']) if pd.notna(row['titre']) else ''\n",
    "            organisme = str(row['organisme']) if pd.notna(row['organisme']) else ''\n",
    "            date_limite = str(row.get('date_limite', '')) if pd.notna(row.get('date_limite')) else ''\n",
    "            combined = f\"{titre}|{organisme}|{date_limite}\"\n",
    "            return hashlib.md5(combined.encode()).hexdigest()[:12]\n",
    "        \n",
    "        mapped_df_ssd['fingerprint'] = mapped_df_ssd.apply(create_fingerprint, axis=1)\n",
    "    \n",
    "    print(f\"‚úÖ DataFrame pr√©par√©: {mapped_df_ssd.shape}\")\n",
    "    print(f\"   Colonnes finales: {list(mapped_df_ssd.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67e70cf",
   "metadata": {},
   "source": [
    "## 5. Fonctions pour extraction PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67657ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_text(pdf_url, max_pages=3):\n",
    "    \"\"\"Extraire le texte d'un PDF depuis une URL\"\"\"\n",
    "    try:\n",
    "        response = requests.get(pdf_url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as tmp:\n",
    "            tmp.write(response.content)\n",
    "            tmp_path = tmp.name\n",
    "        \n",
    "        reader = pypdf.PdfReader(tmp_path)\n",
    "        text = ''\n",
    "        for page_num, page in enumerate(reader.pages[:max_pages]):\n",
    "            text += page.extract_text() + '\\n'\n",
    "        \n",
    "        os.remove(tmp_path)\n",
    "        return text if text.strip() else None\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Erreur PDF {pdf_url}: {str(e)[:50]}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def find_pdf_links(soup, base_url):\n",
    "    \"\"\"Trouver les liens PDF dans une page\"\"\"\n",
    "    pdf_links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link.get('href', '')\n",
    "        text = link.get_text().lower()\n",
    "        \n",
    "        if ('pdf' in href.lower() or \n",
    "            any(keyword in text for keyword in ['reglement', 'document', 'cahier', 'guide', 'annexe'])):\n",
    "            full_url = urljoin(base_url, href)\n",
    "            if full_url not in pdf_links:\n",
    "                pdf_links.append(full_url)\n",
    "    \n",
    "    return pdf_links[:2]\n",
    "\n",
    "print(\"‚úÖ Fonctions PDF cr√©√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e278fa62",
   "metadata": {},
   "source": [
    "## 6. Classe LLMEnricher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5056550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMEnricher:\n",
    "    \"\"\"Enrichir les donn√©es AAP avec Claude Sonnet 4.5\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key=None, model='claude-sonnet-4-5'):\n",
    "        self.api_key = api_key or os.getenv('ANTHROPIC_API_KEY')\n",
    "        self.model = model\n",
    "        self.client = Anthropic(api_key=self.api_key) if self.api_key else None\n",
    "        self.max_retries = 3\n",
    "        self.retry_delay = 1\n",
    "        \n",
    "        if not self.client:\n",
    "            raise ValueError('‚ùå ANTHROPIC_API_KEY non trouv√©e')\n",
    "    \n",
    "    def extract_full_page(self, url, html_content, pdf_texts=None):\n",
    "        \"\"\"Extraire toutes les donn√©es manquantes d'une page avec retry logic\"\"\"\n",
    "        \n",
    "        if not self.client:\n",
    "            return None\n",
    "        \n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        text_content = soup.get_text('\\n')\n",
    "        \n",
    "        if pdf_texts:\n",
    "            text_content += '\\n\\n--- DOCUMENTS PDF ---\\n'\n",
    "            text_content += '\\n\\n'.join(pdf_texts)\n",
    "        \n",
    "        text_content = text_content[:16000]\n",
    "        \n",
    "        prompt = f\"\"\"Tu es un expert en analyse d'appels √† projets fran√ßais (Seine-Saint-Denis).\n",
    "        \n",
    "Analyse cette page et extrais les informations manquantes en JSON valide:\n",
    "\n",
    "{{\n",
    "   \"resume\": \"R√©sum√© en 1-2 phrases (max 300 caract√®res)\",\n",
    "  \"montant_max\": montant maximum en euros (nombre ou null),\n",
    "  \"montant_min\": montant minimum en euros (nombre ou null),\n",
    "  \"taux_financement\": \"pourcentage ou description (null si non trouv√©)\",\n",
    "  \"categories\": [\"liste\", \"de\", \"cat√©gories\"],\n",
    "  \"public_cible\": [\"associations\", \"PME\", \"collectivit√©s\"],\n",
    "  \"mots_cles\": [\"mots-cl√©s\", \"pertinents\"],\n",
    "  \"objectif\": \"Quel est l'objectif principal\",\n",
    "  \"modalite\": \"Conditions principales\",\n",
    "  \"demarches\": \"Comment candidater\",\n",
    "  \"contact\": \"Email ou t√©l√©phone si trouv√© (ou null)\"\n",
    "}}\n",
    "\n",
    "IMPORTANT:\n",
    "- Retourne UNIQUEMENT du JSON valide\n",
    "- Si une info n'existe pas, mets null\n",
    "- Les montants doivent √™tre des nombres\n",
    "- Les listes doivent √™tre des arrays JSON\n",
    "\n",
    "Contenu:\n",
    "{text_content}\"\"\"\n",
    "        \n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                message = self.client.messages.create(\n",
    "                    model=self.model,\n",
    "                    max_tokens=1024,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )\n",
    "                \n",
    "                response_text = message.content[0].text\n",
    "                response_text = response_text.replace('```json', '').replace('```', '')\n",
    "                \n",
    "                return json.loads(response_text.strip())\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"    ‚ùå JSON parsing error: {str(e)[:50]}\")\n",
    "                return None\n",
    "            except Exception as e:\n",
    "                error_msg = str(e)\n",
    "                print(f\"    ‚ùå Attempt {attempt + 1}/{self.max_retries} - Error: {error_msg[:80]}\")\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    wait_time = self.retry_delay * (2 ** attempt)\n",
    "                    print(f\"    ‚è≥ Retrying in {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"    ‚ùå Max retries exceeded\")\n",
    "                    return None\n",
    "\n",
    "print(\"‚úÖ Classe LLMEnricher cr√©√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc26d20",
   "metadata": {},
   "source": [
    "## 7. Initialiser l'enrichisseur LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0305e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    enricher = LLMEnricher()\n",
    "    print(\"‚úÖ LLMEnricher initialis√©\")\n",
    "    print(f\"   Mod√®le: claude-sonnet-4-5\")\n",
    "    print(f\"   Retry logic: Enabled (max 3 attempts with exponential backoff)\")\n",
    "except ValueError as e:\n",
    "    print(f\"‚ùå {str(e)}\")\n",
    "    enricher = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26765193",
   "metadata": {},
   "source": [
    "## 8. Enrichir avec LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b7e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "if enricher and not mapped_df_ssd.empty and len(mapped_df_ssd) > 0:\n",
    "    print(f\"üîÑ Enrichissement de {len(mapped_df_ssd)} appels √† projets...\\n\")\n",
    "    \n",
    "    for idx, row in mapped_df_ssd.iterrows():\n",
    "        url = row.get('url_source')\n",
    "        titre = str(row.get('titre', 'N/A'))[:50]\n",
    "        \n",
    "        if not url or pd.isna(url):\n",
    "            print(f\"‚è≠Ô∏è  [{idx+1}/{len(mapped_df_ssd)}] {titre}: pas d'URL\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"üîÑ [{idx+1}/{len(mapped_df_ssd)}] {titre}...\", end=' ')\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            pdf_links = find_pdf_links(soup, url)\n",
    "            \n",
    "            pdf_texts = []\n",
    "            if pdf_links:\n",
    "                print(f\"(+{len(pdf_links)} PDFs)\", end=' ')\n",
    "                for pdf_url in pdf_links:\n",
    "                    pdf_text = extract_pdf_text(pdf_url)\n",
    "                    if pdf_text:\n",
    "                        pdf_texts.append(pdf_text[:3000])\n",
    "            \n",
    "            extracted = enricher.extract_full_page(url, response.text, pdf_texts)\n",
    "            \n",
    "            if extracted:\n",
    "                for key, value in extracted.items():\n",
    "                    if key in mapped_df_ssd.columns:\n",
    "                        mapped_df_ssd.at[idx, key] = value\n",
    "                print(\"‚úÖ\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  Aucune donn√©e\")\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(\"‚è±Ô∏è  Timeout\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {str(e)[:30]}\")\n",
    "        \n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Enrichissement termin√©!\")\n",
    "else:\n",
    "    if enricher is None:\n",
    "        print(\"‚ùå Enrichisseur LLM non disponible\")\n",
    "    if mapped_df_ssd.empty:\n",
    "        print(\"‚ùå Pas de donn√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5911ffee",
   "metadata": {},
   "source": [
    "## 9. Statistiques et aper√ßu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21a03d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not mapped_df_ssd.empty:\n",
    "    print(\"üìä Statistiques:\")\n",
    "    print(f\"\\n‚úÖ Total: {len(mapped_df_ssd)} enregistrements\")\n",
    "    print(f\"\\n‚úÖ Remplissage:\")\n",
    "    for col in ['titre', 'resume', 'montant_max', 'montant_min', 'categories', 'public_cible', 'mots_cles', 'objectif']:\n",
    "        if col in mapped_df_ssd.columns:\n",
    "            filled = mapped_df_ssd[col].notna().sum()\n",
    "            pct = (filled / len(mapped_df_ssd) * 100) if len(mapped_df_ssd) > 0 else 0\n",
    "            print(f\"   - {col}: {filled}/{len(mapped_df_ssd)} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc43d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not mapped_df_ssd.empty and len(mapped_df_ssd) > 0:\n",
    "    print(\"\\nüìã Aper√ßu:\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    for idx in range(min(3, len(mapped_df_ssd))):\n",
    "        row = mapped_df_ssd.iloc[idx]\n",
    "        print(f\"\\nüìå {row['titre'][:60]}\")\n",
    "        print(f\"   Org: {row['organisme']}\")\n",
    "        if pd.notna(row.get('date_publication')):\n",
    "            print(f\"   Date pub: {row['date_publication']}\")\n",
    "        if pd.notna(row.get('montant_max')):\n",
    "            print(f\"   Montant max: {row['montant_max']}‚Ç¨\")\n",
    "        if pd.notna(row['resume']):\n",
    "            print(f\"   R√©sum√©: {str(row['resume'])[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe7c600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration affichage Pandas optimis√©e\n",
    "if not mapped_df_ssd.empty:\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_colwidth', 100)\n",
    "    pd.set_option('display.width', None)\n",
    "    \n",
    "    print(\"\\nüìä VUE COMPL√àTE DU DATAFRAME\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Vue tabulaire\n",
    "    display(mapped_df_ssd.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa56e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vue HTML interactive\n",
    "if not mapped_df_ssd.empty:\n",
    "    from IPython.display import HTML\n",
    "    display(HTML(mapped_df_ssd.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392518da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vue transpos√©e (une ligne = une colonne)\n",
    "if not mapped_df_ssd.empty:\n",
    "    display(mapped_df_ssd.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c52f26",
   "metadata": {},
   "source": [
    "## 10. Exporter les donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd1d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not mapped_df_ssd.empty:\n",
    "    # Exporter en CSV\n",
    "    csv_output = '../data/seine_saint_denis_aap_enriched.csv'\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(csv_output), exist_ok=True)\n",
    "        mapped_df_ssd.to_csv(csv_output, index=False, encoding='utf-8')\n",
    "        print(f\"‚úÖ Export√© en CSV: {csv_output}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Erreur CSV: {str(e)}\")\n",
    "    \n",
    "    # Exporter en JSON\n",
    "    json_output = '../data/seine_saint_denis_aap_enriched.json'\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(json_output), exist_ok=True)\n",
    "        df_for_json = mapped_df_ssd.copy()\n",
    "        for col in df_for_json.columns:\n",
    "            if df_for_json[col].dtype == 'object':\n",
    "                df_for_json[col] = df_for_json[col].astype(str)\n",
    "        \n",
    "        df_for_json.to_json(json_output, orient='records', force_ascii=False, indent=2)\n",
    "        print(f\"‚úÖ Export√© en JSON: {json_output}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Erreur JSON: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4680b434",
   "metadata": {},
   "source": [
    "## 11. Upload Airtable (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b511552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Export to CSV\n",
    "output_path = r'c:\\Users\\WALID\\Documents\\Code\\appels-a-projets\\filtered_projects.csv'\n",
    "df_projects.to_csv(output_path, index=False, encoding='utf-8')\n",
    "print(f\"Results exported to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
